# Contributing

*Please note that this project is released with a [Contributor Code of Conduct](CODE_OF_CONDUCT.md). By participating in this project you agree to abide by its terms.*

Since Di√°rio Oficial seeks to become a trustful source of official governmental announcements, every data source must also be trustful. A private company publishing government gazettes without official permission, for instance, is not a trustful source.

At this moment, the project aims for the 100 top Brazilian municipalities by population. If you want to help us reach this goal, [an ordered list can be found on Wikipedia](https://pt.wikipedia.org/wiki/Lista_de_munic%C3%ADpios_do_Brasil_por_popula%C3%A7%C3%A3o).

We're tracking the progress of crawlers and parser in the [CITIES.md](https://github.com/okfn-brasil/diario-oficial/blob/master/CITIES.md) file. Take a look there before starting to write any source code.

## Development

If you're not familiar with [Scrapy](https://docs.scrapy.org), read the it's documentation and follow the [tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html) before writing any code.

To run your crawler locally, run the command bellow passing the name of the crawler you want to execute:

```sh
$ docker-compose run --rm processing bash -c "cd data_collection && scrapy crawl <CRAWLER_NAME>"
```

### Extracting data

Use the [Scrapy shell](https://doc.scrapy.org/en/latest/intro/tutorial.html#extracting-data) to test queries on the web pages to extract data. It opens an interactive shell so you can easilly test queries to extract data:

```sh
docker-compose run --rm processing scrapy shell 'https://www.joinville.sc.gov.br/jornal/index/page/1'
```

## Automated Testing

The project is backed by a test suite, which can be run with a single command.

```sh
$ make test
```
